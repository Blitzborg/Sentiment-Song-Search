# -*- coding: utf-8 -*-
"""Explicit Word Removal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17kWs69WgRZNxt3h31cVMhPiwhdVkRqLV
"""

from nltk.corpus import stopwords  
from nltk.tokenize import word_tokenize  
from nltk.tokenize import RegexpTokenizer
import re 
import os

nltk.download('stopwords')
nltk.download('punkt')

tokenizer = RegexpTokenizer(r'\w+')
stop_words = set(stopwords.words('english'))

def tokenise(corpus):
  filtered_sentence = [] 
  word_tokens = tokenizer.tokenize(corpus) 
  filtered_sentence = [w for w in word_tokens if not w in stop_words]  
  return filtered_sentence

def map_book(tokens):
    hash_map = {}

    if tokens is not None:
        for element in tokens:          
            if element in hash_map:
                hash_map[element] = hash_map[element] + 1
            else:
                hash_map[element] = 1

        return hash_map
    else:
        return None
def initialize(path):
      filename = (path)
      f = open(filename)
      wordlist = f.readlines()
      wordlist = [w.strip() for w in wordlist if w]
      return wordlist

def load_corpus(path):
    tot = ""
    filename = (path)
    f = open(filename)
    wordlist = f.readlines()
    tot += str(wordlist)
    return tot

corpus = load_corpus('/content/look_at_me_now.txt')
word_list = initialize('/content/wordlist.txt')
words = tokenise(corpus)

map = map_book(words)
explicit_count = 0
explicit_word = {}
for word in word_list:
    try:
          explicit_count +=map[word]
          # print('Word: [' + word + '] Frequency: ' + str(map[word]))
          print("Explicit Count: "+ str(explicit_count))
          explicit_word[word] = map[word]
    except:
          continue
print(explicit_word)

values = map.values()
tot = sum(values)
exp_values = explicit_word.values()
exp_tot = sum(exp_values)
print("Explicit Rating: "+ str(exp_tot/tot * 400 ) if  (exp_tot/tot * 400 < 100) else str(100.00) + '%')

